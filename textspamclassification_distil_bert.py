# -*- coding: utf-8 -*-
"""textSpamClassification_distil_BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KNK5wNfjfNApQG7j_QKfRilBvwFdgUEZ

#  distilBERT base model Fine-tuning usingLoRA

Classifying spam emails/texts using distilBERT.
"""

!pip install peft
!pip install evaluate
!pip install transformers[torch]
!pip install accelerate -U
!pip install transformers -U

import accelerate
print(accelerate.__version__)

import numpy as np
import pandas as pd
import tensorflow as tf
import peft
from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig
import evaluate
import torch
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, Trainer, TrainingArguments
import evaluate



"""## Import dataset and split them"""

# Assuming df is your initial DataFrame
df=messages = pd.read_csv('Texts', sep='\t',
                           names=["label", "message"])

# First split into training and test/validation sets
train_df, test_df = train_test_split(df, test_size=0.2)  # 80% training, 20% test/validation

# Further split test/validation into test and validation sets
train_df, val_df = train_test_split(train_df, test_size=0.2)  # Splitting the 30% into two parts

"""## Model"""

from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, DataCollatorWithPadding,TrainingArguments, Trainer

model_checkpoint = 'distilbert-base-uncased'
# model_checkpoint = 'roberta-base' # you can alternatively use roberta-base but this model is bigger thus training will take longer

# define label maps
id2label = {0: "ham", 1: "spam"}
label2id = {"ham":0, "spam":1}

# generate classification model from model_checkpoint
model = AutoModelForSequenceClassification.from_pretrained(
    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)

"""### Tokenizer"""

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

from datasets import Dataset
# Tokenization function
def tokenize_and_format(df):
    # Tokenize all texts
    tokenized_inputs = tokenizer(df['message'].tolist(), padding='max_length', truncation=True, max_length=512)

    # Ensure labels are integers
    if df['label'].dtype == 'object':  # Adjust if your labels are not object type
        df['label'] = df['label'].map(label2id)  # Convert labels using label map

    # Convert to DataFrame
    tokenized_df = pd.DataFrame(tokenized_inputs.data)
    tokenized_df['labels'] = df['label'].tolist()

    # Convert DataFrame to Hugging Face Dataset
    return Dataset.from_pandas(tokenized_df)

# Prepare datasets
train_dataset = tokenize_and_format(train_df)
val_dataset = tokenize_and_format(val_df)
test_dataset = tokenize_and_format(test_df)

"""# data collator will dynamically pad examples in each batch to be equal length"""

# create data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

"""### define an evaluation function to pass into trainer later"""

# define an evaluation function to pass into trainer later
from datasets import load_metric
# Load the accuracy metric
accuracy_metric = evaluate.load("accuracy")

# Define the compute_metrics function
def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=1)
    return {"accuracy": accuracy_metric.compute(predictions=predictions, references=labels)["accuracy"]}

"""#### LORA"""

peft_config = LoraConfig(task_type="SEQ_CLS",
                        r=4,
                        lora_alpha=32,
                        lora_dropout=0.01,
                        target_modules = ['q_lin'])

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

"""### Training Arguments"""

# hyperparameters
lr = 1e-3
batch_size = 4
num_epochs = 5

# define training arguments
training_args = TrainingArguments(
    output_dir= model_checkpoint + "-lora-text-classification",
    learning_rate=lr,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_epochs,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

"""### Set the trainer object
### Train the model
"""

# creater trainer object
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length
    compute_metrics=compute_metrics,
)

# train model
trainer.train()

# Evaluate the model on the test dataset
trainer.evaluate(test_dataset)

